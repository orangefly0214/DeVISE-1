{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By Kevin Xu\n",
    "#kevin28520@gmail.com\n",
    "\n",
    "\n",
    "#DATA:\n",
    "    #1. cifar10(binary version):https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "    #2. pratrained weights (vgg16.npy):https://mega.nz/#!YU1FWJrA!O1ywiCS2IiOlUCtCpI6HTJOMrneN-Qdv3ywQP5poecM\n",
    "    \n",
    "# TO Train and test:\n",
    "    #0. get data ready, get paths ready !!!\n",
    "    #1. run training_and_val.py and call train() in the console\n",
    "    #2. call evaluate() in the console to test\n",
    "    \n",
    "#\n",
    "\n",
    "\n",
    "#! /usr/bin/env python   \n",
    "# -*- coding: utf-8 -*-  \n",
    "\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import input_data\n",
    "import VGG\n",
    "import tools\n",
    "import time\n",
    "#\n",
    "IMG_W = 32\n",
    "IMG_H = 32\n",
    "N_CLASSES = 10\n",
    "BATCH_SIZE = 32\n",
    "learning_rate = 0.01\n",
    "MAX_STEP = 100   # it took me about one hour to complete the training.\n",
    "IS_PRETRAIN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image as im\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "\n",
    "pic = \"/home/tony/Desktop/Datasets/cifar_10_batches_py/batches.meta\"\n",
    "data_dir = \"/home/tony/Desktop/Datasets/cifar_10_batches_py/data_batch_\"\n",
    "\n",
    "meta = unpickle(pic)  #data format  -->  dictionary\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = [ (data_dir + str(i)) for i in np.arange(1, 5)]\n",
    "val_path = \"/home/tony/Desktop/Datasets/cifar_10_batches_py/data_batch_5\"\n",
    "test_path =  \"/home/tony/Desktop/Datasets/cifar_10_batches_py/test_batch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tony/Desktop/Datasets/cifar_10_batches_py/test_batch\n",
      "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n"
     ]
    }
   ],
   "source": [
    "print(test_path)\n",
    "\n",
    "x = unpickle(test_path)\n",
    "\n",
    "print(x.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[158, 159, 165, ..., 124, 129, 110],\n",
       "       [235, 231, 232, ..., 178, 191, 199],\n",
       "       [158, 158, 139, ...,   8,   3,   7],\n",
       "       ...,\n",
       "       [ 20,  19,  15, ...,  50,  53,  47],\n",
       "       [ 25,  15,  23, ...,  80,  81,  80],\n",
       "       [ 73,  98,  99, ...,  94,  58,  26]], dtype=uint8)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[b'data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    t0 = time.time()\n",
    "    pre_trained_weights ='./vgg16_pretrain/vgg16.npy'\n",
    "    train_log_dir = \"./my_train_logs/\"\n",
    "    train_log_dir = './my_train_logs/'\n",
    "    val_log_dir = './my_train_logs/val/'\n",
    "    \n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=True): \n",
    "        x = tf.placeholder(tf.float32, shape=[None, IMG_W, IMG_H, 3])\n",
    "        y_ = tf.placeholder(tf.int16, shape=[None, N_CLASSES]) \n",
    "\n",
    "        logits = VGG.VGG16N(x, N_CLASSES, IS_PRETRAIN)\n",
    "        loss = tools.loss(logits, y_)\n",
    "        accuracy = tools.accuracy(logits, y_)\n",
    "\n",
    "        my_global_step = tf.Variable(0, name='global_step', trainable=False) \n",
    "        train_op = tools.optimize(loss, learning_rate, my_global_step)   \n",
    "\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        summary_op = tf.summary.merge_all()   \n",
    "\n",
    "\n",
    "\n",
    "        val_image_batch = unpickle(val_path)[b'data']\n",
    "        val_image_batch = np.reshape(val_image_batch,(-1,3,32,32))\n",
    "        val_image_batch = np.transpose(val_image_batch,[0,2,3,1])\n",
    "\n",
    "        val_label_batch = unpickle(val_path)[b'labels']\n",
    "        val_label_batch_tmp = np.max(val_label_batch)+1\n",
    "        val_label_batch = np.eye(val_label_batch_tmp)[val_label_batch]\n",
    "                    \n",
    "\n",
    "        t1 = time.time()\n",
    "        print(\"t1-t0 time = %f\",t1-t0)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # load the parameter file, assign the parameters, skip the specific layers\n",
    "        tools.load_with_skip(pre_trained_weights, sess, ['fc6','fc7','fc8'])   \n",
    "\n",
    "        tra_summary_writer = tf.summary.FileWriter(train_log_dir, sess.graph)\n",
    "        val_summary_writer = tf.summary.FileWriter(val_log_dir, sess.graph)\n",
    "        \n",
    "        \n",
    "        t2 = time.time()\n",
    "        print(\"t2-t1 time = %f\",t2-t1)\n",
    "        \n",
    "        print(\"start training\")\n",
    "    \n",
    "        try:\n",
    "\n",
    "            for epoch in range(MAX_STEP):\n",
    "                for batch_path in train_path:\n",
    "\n",
    "                    \n",
    "                    t3 = time.time()\n",
    "                    print(\"t2-t1 time = %f\",t3-t2)\n",
    "                    \n",
    "                    print(\"step %d\"%epoch)\n",
    "\n",
    "                    tra_images = unpickle(batch_path)[b'data']\n",
    "                    tra_images = np.reshape(tra_images,(-1,3,32,32))\n",
    "                    tra_images = np.transpose(tra_images,[0,2,3,1])\n",
    "\n",
    "                    tra_labels = unpickle(batch_path)[b'labels']\n",
    "                    tra_labels_tmp = np.max(tra_labels)+1\n",
    "                    tra_labels = np.eye(tra_labels_tmp)[tra_labels]\n",
    "                    \n",
    "                    \n",
    "                    _, tra_loss, tra_acc = sess.run([train_op, loss, accuracy],\n",
    "                                                    feed_dict={x:tra_images, y_:tra_labels})            \n",
    "                    if epoch % 50 == 0 or (epoch + 1) == MAX_STEP:                 \n",
    "                        print ('Step: %d, loss: %.4f, accuracy: %.4f%%' % (epoch, tra_loss, tra_acc))\n",
    "                        # summary_str = sess.run(summary_op)\n",
    "                        # tra_summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "                    if epoch % 20 == 0 or (epoch + 1) == MAX_STEP :\n",
    "                        val_loss, val_acc = sess.run([loss, accuracy],\n",
    "                                                     feed_dict={x:val_image_batch,y_:val_label_batch})\n",
    "                        print('**  Step %d, val loss = %.2f, val accuracy = %.2f%%  **' %(epoch, val_loss, val_acc))\n",
    "\n",
    "                        # summary_str = sess.run(summary_op)\n",
    "                        # val_summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "                    if epoch % 20 == 0 or (epoch + 1) == MAX_STEP:\n",
    "                        checkpoint_path = os.path.join(train_log_dir, 'model.ckpt')\n",
    "                        saver.save(sess, checkpoint_path, global_step=epoch)\n",
    "                    t4 = time.time()\n",
    "                    print(\"t4-t3 time = %f\",t4-t3)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done training -- epoch limit reached')\n",
    "        finally:\n",
    "            sess.close()\n",
    "        \n",
    "        t4 = time.time()\n",
    "        print(\"t4-t3 time = %f\",t4-t3)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1-t0 time = %f 2.571227550506592\n",
      "t2-t1 time = %f 77.40646386146545\n",
      "start training\n",
      "t2-t1 time = %f 0.0017774105072021484\n",
      "step 0\n",
      "Step: 0, loss: 4.4414, accuracy: 11.8800%\n",
      "**  Step 0, val loss = 2.32, val accuracy = 9.60%  **\n",
      "t4-t3 time = %f 581.3157982826233\n",
      "t2-t1 time = %f 581.3176867961884\n",
      "step 0\n",
      "Step: 0, loss: 2.3197, accuracy: 9.6300%\n",
      "**  Step 0, val loss = 2.30, val accuracy = 10.23%  **\n",
      "t4-t3 time = %f 325.7335755825043\n",
      "t2-t1 time = %f 907.051361322403\n",
      "step 0\n",
      "Step: 0, loss: 2.3012, accuracy: 10.0800%\n",
      "**  Step 0, val loss = 2.30, val accuracy = 10.46%  **\n",
      "t4-t3 time = %f 288.79326343536377\n",
      "t2-t1 time = %f 1195.8518834114075\n",
      "step 0\n",
      "Step: 0, loss: 2.3013, accuracy: 10.1400%\n",
      "**  Step 0, val loss = 2.30, val accuracy = 10.60%  **\n",
      "t4-t3 time = %f 282.1490046977997\n",
      "t2-t1 time = %f 1478.0223174095154\n",
      "step 1\n",
      "t4-t3 time = %f 412.4128520488739\n",
      "t2-t1 time = %f 1890.4768891334534\n",
      "step 1\n",
      "t4-t3 time = %f"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
